---
title: "STAT420 Zobreth Group Project"
author: "Brian Betancourt, Ethan Cook, Zongyu Li"
date: "2023-07-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# North American Video Game Sales Prediction

## Introduction
In this project, we assume the role of a video game company seeking to release games in the North American market for the first time. Given historical data regarding video game sales from other companies, we will use this data to predict our own successes (or follies) in the North American market and provide an informed decision to our company leadership regarding the potential risks in doing so.

### Exploring The Data

Required library, data imports.
```{r}
library(faraway)
library(car)
library(ggplot2)
#install.packages("tidyverse") # uncomment if needing to download the package first
library(tidyverse)
vgsales = read.csv("vgsales.csv")
```

The data is stored as a `csv` file containing `r nrow(vgsales)` rows across `r ncol(vgsales)` columns (or predictors), and includes data from 1980 to 2020.


The data is largely "clean" and does not contain missing or "null" values, with the exception of the `Year` column which contains approximately `1.63%` null values.

```{r}
# null values per column
na_count = colSums(is.na(vgsales))
# proportion of NA values in each column
na_proportion = na_count / nrow(vgsales) * 100
# table/dataframe for readability
na_records = data.frame(Count = na_count, Proportion = na_proportion)
print(na_records)
```

We will also check for any unconventional data (i.e. non-numeric or date types in the `Year` predictor). 

Given that there appear to be a low number of records (271, or approximately 1.63% of the total dataset), we cast `Year` to be a numeric datatype for ease of use in modeling and we filter our dataset to remove these N/A records.
```{r}
# check non-numeric values in Year
unique(vgsales$Year[!grepl("^\\d+$", vgsales$Year)])

# count of "N/A" values in Year column
na_count = sum(vgsales$Year == "N/A")
# proportion of "N/A" values in Year column
na_proportion = na_count / nrow(vgsales) * 100

# print count and proportion
print(paste("Count of 'N/A' in Year: ", na_count))
print(paste("Proportion of 'N/A' in Year (%): ", na_proportion))

# filter the dataset
vgsales = vgsales[vgsales$Year != "N/A",]

# cast datatype
vgsales$Year = as.numeric(vgsales$Year)
```

It appear also that our `Year` values seem to trail off after 2016, so we can remove records from the dataset where year is > 2016
```{r}
# aggregate by year
global_sales_year = aggregate(Global_Sales ~ Year, vgsales, sum)

# plot
barplot(height = global_sales_year$Global_Sales, names.arg = global_sales_year$Year, xlab = "Year", ylab = "Total Sales (in millions)",main = "Total Sales (Global) by Year", las = 2,cex.names = 0.7) 

# filter dataset
vgsales = subset(vgsales, Year <= 2016)

```

The predictors have the following datatypes:

```{r}
str(vgsales)
```
- **Rank**: 
- *Name*: This is the title of a given video game
- *Platform*: This corresponds to the console the video game is released for
**Note**: It is possible that a video game may be released for more than one console simultaneously.
- *Year*: The year the video game was released
- *Publisher*: Company which published the video game
- *NA_Sales*: Number of sales in North America (in *millions of units sold*)
- *EU_Sales*: Number of sales in Europe (in *millions of units sold*)
- *JP_Sales*: Number of sales in Japan (in *millions of units sold*)
- *Other_Sales*: Number of sales in regions of the world outside of North America, Europe, and Japan (in *millions of units sold*)
- *Global_Sales*: Total Number of sales, globally (in *millions of units sold*)

#### Data Tendency

*What do sales look like, on average, for each geographic area and based on platform?*
```{r}
mean_sales_platform = aggregate(cbind(EU_Sales, JP_Sales, Other_Sales) ~ Platform, vgsales, mean)
print(mean_sales_platform)
```

**Assuming our company only cares about platforms from 2010 onward, we can focus on newer systems**

Below we can observe the average sales for each platform from 2010 to 2016
```{r}
# utilizing domain knowledge to filter platforms to only newer ones
new_platforms = c('X360', 'Wii', 'WiiU', 'XOne', 'PS3', 'PS4', 'PSV', '3DS', 'PC')
# filter data frame to include only specified platforms
vgsales_filtered = vgsales[vgsales$Platform %in% new_platforms, ]

mean_sales_platform_filtered = aggregate(cbind(EU_Sales, JP_Sales, Other_Sales) ~ Platform, vgsales_filtered, mean)
print(mean_sales_platform_filtered)

```


Of the newer platforms, which are most popular in each geographic region?
```{r}
# Find the platforms with highest mean sales in each region
highest_EU_platform = mean_sales_platform_filtered$Platform[which.max(mean_sales_platform_filtered$EU_Sales)]
highest_JP_platform = mean_sales_platform_filtered$Platform[which.max(mean_sales_platform_filtered$JP_Sales)]
highest_Other_platform = mean_sales_platform_filtered$Platform[which.max(mean_sales_platform_filtered$Other_Sales)]

highest_EU_sales = max(mean_sales_platform_filtered$EU_Sales)
highest_JP_sales = max(mean_sales_platform_filtered$JP_Sales)
highest_Other_sales <- max(mean_sales_platform_filtered$Other_Sales)

cat("Platform with highest mean sales in EU: ", highest_EU_platform, " (", highest_EU_sales, ")", "\n")
cat("Platform with highest mean sales in JP: ", highest_JP_platform, " (", highest_JP_sales, ")", "\n")
cat("Platform with highest mean sales in Other regions: ", highest_Other_platform, "(", highest_Other_sales, ")", "\n")
```

#### Data Distribution


Sales by Genre
```{r}

# Aggregate total sales
eusales_agg = aggregate(EU_Sales ~ Genre, vgsales, sum)
jpsales_agg = aggregate(JP_Sales ~ Genre, vgsales, sum)
othersales_agg = aggregate(Other_Sales ~ Genre, vgsales, sum)

# EU Sales
ggplot(eusales_agg, aes(x = Genre, y = EU_Sales)) +geom_bar(stat = "identity", fill = "lightblue", color = "black") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x = "Genre", y = "Sales (in millions)", title = "Total EU Sales by Genre")

# JP Sales
ggplot(jpsales_agg, aes(x = Genre, y = JP_Sales)) + geom_bar(stat = "identity", fill = "lightblue", color = "black") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x = "Genre", y = "Sales (in millions)", title = "Total JP Sales by Genre")

# Other Sales
ggplot(othersales_agg, aes(x = Genre, y = Other_Sales)) + geom_bar(stat = "identity", fill = "lightblue", color = "black") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x = "Genre", y = "Sales (in millions)", title = "Total Other Sales by Genre")

```



### Data Trends

Total Sales by Year and Geographic Region
```{r}
# convert year (currently chr)
vgsales$Year = as.numeric(vgsales$Year)

# 2010 onward filter
vgsales_current = vgsales[vgsales$Year >= 2010, ]

# Calculate total sales using the filtered data frame
total_sales_year = aggregate(cbind(EU_Sales, JP_Sales, Other_Sales) ~ Year, vgsales_current, sum)

ggplot(total_sales_year, aes(x = as.numeric(Year))) + geom_line(aes(y = JP_Sales, color = "JP_Sales")) + geom_line(aes(y = EU_Sales, color = "EU_Sales")) + geom_line(aes(y = Other_Sales, color = "Other_Sales")) + labs(color = "Region", x="Year", y = "Total Sales", title = "Yearly Sales by Region")

```


Total Sales by Platform (2010-2020)
```{r}

# year filter
vgsales_current = vgsales[vgsales$Year >= 2010 & vgsales$Year <= 2020, ]
total_sales_platform = aggregate(cbind(EU_Sales, JP_Sales, Other_Sales) ~ Platform, vgsales_current, sum)

ggplot(total_sales_platform, aes(x = Platform, y = JP_Sales)) + geom_bar(stat="identity") + theme(axis.text.x = element_text()) + labs(title = "Total JP Sales by Platform (2010-2020)", x = "Platform", y = "Sales")

ggplot(total_sales_platform, aes(x = Platform, y = EU_Sales)) + geom_bar(stat="identity") + theme(axis.text.x = element_text()) + labs(title = "Total EU Sales by Platform (2010-2020)", x = "Platform", y = "Sales")

ggplot(total_sales_platform, aes(x = Platform, y = Other_Sales)) + geom_bar(stat="identity") + theme(axis.text.x = element_text()) + labs(title = "Total Other Region Sales by Platform (2010-2020)", x = "Platform", y = "Sales")


```

## Methods

### Pre-Processing
Several of our potential predictors are represented as characters, but we will want to convert them to factor variables. These include:  
* Platform  
* Genre  
* Publisher  
  
Previously we have filtered our dataset to only include games and platforms from 2010 on; next we'll make the character predictors into factors, then drop levels from factor variables that have no observations (i.e. levels that are only associated with data before 2010, such as Super Nintendo game data).

```{r}
# Factorize string predictors
vgsales_current$Platform = as.factor(vgsales_current$Platform)
vgsales_current$Genre = as.factor(vgsales_current$Genre)
vgsales_current$Publisher = as.factor(vgsales_current$Publisher)

# Remove unused factor levels
vgsales_modern = droplevels(vgsales_current)
```

```{r}
# run a correlation matrix on only the numeric predictors
numeric_cols = sapply(vgsales_modern, is.numeric)
numeric_vgsales_modern = vgsales_modern[,numeric_cols]
cor(numeric_vgsales_modern)
```

We have also decided to combine EU_Sales, JP_Sales, and Other_Sales into a new predictor called Non_NA_Sales. This is because sales in different countries are correlated (see correlation table above), and Global_Sales is highly correlated with the others since it is an aggregation, which also includes NA_Sales. I am removing the combined columns for better viewing.

```{r}
# combine sales predictors into one
Non_NA_Sales = vgsales_modern$EU_Sales + vgsales_modern$JP_Sales + vgsales_modern$Other_Sales
vgs_modern = cbind(vgsales_modern, Non_NA_Sales)
vgs_modern = subset(vgsales_modern, select = -c(EU_Sales, JP_Sales, Other_Sales))

str(vgs_modern)
```

*Note:* The Year predictor will be tried as a numerical variable, but for our smaller range of years a factor might be more appropriate.

### Model Selection
*In this section, we choose a model which can represent the data most accurately*
- Perform any tests necessary to find correlation between variables (i.e. correlation matrix)

We will generate several models and compare metrics to give us a starting point on intuition. For all models, `NA_Sales` is the response term and represents the units sold in North America.

```{r}
all_intr_mod = lm(NA_Sales ~ Platform * Year * Genre * Non_NA_Sales + Publisher, data = vgs_modern)
all_mod = lm(NA_Sales ~ Platform + Year + Genre + Publisher + Non_NA_Sales, data = vgs_modern)
foreign_sales_mod = lm(NA_Sales ~ Non_NA_Sales, data = vgs_modern)
intuitive_mod = lm(NA_Sales ~ Non_NA_Sales * Platform + Non_NA_Sales * Genre + Year, data = vgs_modern)
intuitive_mod2 = lm(NA_Sales ~ Non_NA_Sales * Platform + Non_NA_Sales * Genre + Non_NA_Sales * Publisher + Year, data = vgs_modern)
bic_back_mod = step(all_intr_mod, direction = "backward", trace = FALSE, k = log(nrow(vgs_modern)))
```

The intuitive models above were chosen using experience as a customer of video games and a previous salesman. "intuitive_mod", posits that there are interactions between global non-NA sales and the platform/genre of the game. Platforms and genres can vary a lot in popularity and so I am accounting for different coefficients needed for the different factor levels.

The 2nd intuitive model, "intuitive_mod2", is similar to the first but includes an interaction between global non-NA sales and the video game publisher. This is based off of the understanding that there are big-name publishers that always produce more sales (profit not being a factor) and smaller independent publishers with more modest sales. As seen in later tests, this interaction gives the model a large boost in prediction accuracy without making it much more confusing.

With our models generated, let's get information on violated assumptions before producing results. I'll use the two models with the best prediction and interpretability combinations (metrics on prediction in later charts).
```{r}
par(mfrow = c(1, 2))
plot(fitted(intuitive_mod2), resid(intuitive_mod2), xlab = "Fitted", ylab = "Residuals", main = "Intuitive Model 2 Fit vs. Resid")
plot(fitted(bic_back_mod), resid(bic_back_mod), xlab = "Fitted", ylab = "Residuals", main = "BIC Backward Model Fit vs. Resid")
```

We see similar charts where there is a lot of variability regardless of fitted magnitude, but the less popularly selling games tend to be easier to predict. This also shows that the distribution of our data is far from normal, which we'll confirm below.

Now we will test the assumptions of a SLR model. Generating a Q-Q plot for each model yeilds very similar results, so I will only include the graph for the better performing intuitive model 2:
```{r}
qqnorm(resid(intuitive_mod2))
qqline(resid(intuitive_mod2), col = "red", lwd = 2)
stest = shapiro.test(resid(intuitive_mod2)[1:5000])
stest
```
Clearly, the errors of our model are not normally distributed, as the plot shows trailing tails at both ends. The Shapiro-Wilkes test concurs by giving an extremely small p-value of `r stest$p.value`. *Note: we had to exclude 141 of the 5141 residuals dues to limitations of the shapiro.test function*.

## Results
In the following figures, we show bar plots for the R^2 values and RMSE. We observe that for “all_intr_mod”, “bic_back_mod” and “intuitive_mod” and “intuitive_mod2" have the highest R^2 values. For the RMSE bar plot, these 4 models also have the lowest RMSE.

```{r}
# Given data
data <- data.frame(
  Model = c("all_intr_mod", "all_mod", "foreign_sales_mod", "intuitive_mod", "intuitive_mod2", "bic_back_mod"),
  R_Squared = c(summary(all_intr_mod)$adj.r.squared, 
              summary(all_mod)$adj.r.squared, 
              summary(foreign_sales_mod)$adj.r.squared, 
              summary(intuitive_mod)$adj.r.squared, 
              summary(intuitive_mod2)$adj.r.squared,
              summary(bic_back_mod)$adj.r.squared),
  RMSE = c(sqrt(mean(resid(all_intr_mod) ^ 2)), 
            sqrt(mean(resid(all_mod) ^ 2)), 
            sqrt(mean(resid(foreign_sales_mod) ^ 2)), 
            sqrt(mean(resid(intuitive_mod) ^ 2)), 
            sqrt(mean(resid(intuitive_mod2) ^ 2)), 
            sqrt(mean(resid(bic_back_mod) ^ 2)))
)

# Reshape the data to a long format using gather (tidyr)
data_long <- data %>%
  gather(metric, value, -Model)

# Bar plot for R_Squared
ggplot(data_long[data_long$metric == "R_Squared", ], aes(x = Model, y = value)) +
  geom_col(fill = "blue") +
  labs(x = "Model", y = "R_Squared") +
  ggtitle("R_Squared for Different Models") +
  theme_minimal()

# Bar plot for RMSE
ggplot(data_long[data_long$metric == "RMSE", ], aes(x = Model, y = value)) +
  geom_col(fill = "green") +
  labs(x = "Model", y = "RMSE") +
  ggtitle("RMSE for Different Models") +
  theme_minimal()

```

Looking at the above chart's values, we see the large model with many fully interacting predictors has the best metrics. This is unsurprising, since adding more and more predictors and interactions generally produces better predictions, but can become confusing and loses ability to interpret meaning between variables.

To conduct a comprehensive comparison of the four models, namely "all_intr_mod", "bic_back_mod" and "intuitive_mod" and "bic_back_mod2". We conducted ANOVA tests on each pair of them. From the following tables, we see that all p-values are significant. A positive "Sum of Sq" indicates that Model 1 explains more variance than Model 2. Conversely, a negative "Sum of Sq" indicates that Model 2 explains more variance than Model 1. By referring to the table, we have "all_intr_mod" having better performance than "bic_back_mod", "intuitive_mod" and "intuitive_mod2". Then we have "intuitive_mod2" having statically better performance than "bic_back_mod" and "intuitive_mod1". In the end, we have "bic_back_mod" having better performance than "intuitive_mod1". In summary, we can rank the best-performing modeling to the least-performance model as follows "all_intr_mod">"intuitive_mod2">"bic_back_mod">"intuitive_mod1". 

```{r}


# Perform ANOVA between all_intr_mod and bic_back_mod
anova1 <- anova(all_intr_mod, bic_back_mod) #(all intr)

# Perform ANOVA between all_intr_mod and intuitive_mod
anova2 <- anova(all_intr_mod,intuitive_mod) # (all_intro)

# Perform ANOVA between all_intr_mod and intuitive_mod2
anova3 <- anova(all_intr_mod, intuitive_mod2)  # (all_intro)

# Perform ANOVA between bic_back_mod and intuitive_mod
anova4 <- anova(bic_back_mod,intuitive_mod) # (back)

# Perform ANOVA between bic_back_mod and intuitive_mod2
anova5 <- anova(bic_back_mod,intuitive_mod2) # (intuitive_mod2)

# Perform ANOVA between intuitive_mod and intuitive_mod2
anova6 <- anova(intuitive_mod, intuitive_mod2) # (intuit 2)

# Extract ANOVA results
print(anova1)
print(anova2)
print(anova3)
print(anova4)
print(anova5)
print(anova6)

```

The "all_intr_mod" model's improved performance can be attributed to its inclusion of the interaction terms, which allowed for a more accurate and nuanced representation of the relationship between the predictors and the response variable, NA_Sales. 


## Discussion
In the context of our original business problem, the statistical results were generated to support our interest of our video game company to enter the North American market. Our most predictive model was `all_intr_mod`, the most advanced model which utilized all possible interactions.

The model itself is complex, with test statistics calculated for each possible term interaction. This presents issues in terms of interpretability, and its' accuracy relative to other models can be considered a trade-off when compared to the relative simplicity of a more intuitive model with an acceptable level of accuracy.

Taking a look at the test statistics and associated P-Values for the predictors in this model, we can attempt to take a peek into what might be considered a "translucent box" in terms of the model's inner workings. For example, we can see quite a few interactions which have positive Estimates and statistically significant P-Values (again, assuming $\alpha=0.05$) such as:
- Year:GenreRole-Playing:Non_NA_Sales
- PlatformX360
- PlatformWii:GenreMisc
These findings suggest that:
- Role Playing Games correlate significantly with sales outside of North America, and potentially in an increasing fashion over time.
- Xbox 360 generally correlates positively North American sales (note that this is a model interpretation and not a hard-and-fast truth).
- Miscellaneous genre games on the Wii console correlate positively with North American sales.

While these findings may be interesting, we can also examine the results from another model we tested: `bic_back_mod`, the model chosen using backwards stepwise predictor selection with Bayesian Information Criterion (BIC). 


In a similar fashion to our most predictive model, we can note positive Estimates and significant P-Values for various predictors:
- PlatformWii
- GenreSports
- GenreShooter
These findings suggest that games on the Wii platform, as well as games which are of the genres "Sports" or "Shooter" were found to be highly predictive in estimating sales. 

Several interactions which utilized the `Year` predictor were also found to be statistically significant, such as:
- PlatformWii:Year
- PlatformPS3:Year:Non_NA_Sales
- PlatformPS4:Year:Non_NA_Sales
The slightly-positive Estimates for these terms possibly suggest that sales increased with subsequent years for these platforms, outside of North America.

However, it is important to restrain our interpretations with the knowledge that linear statistical models function predicated on the assumption of "LINE": Linearity, Independence, Normality, and Equal Variance (homoscedasticity). Based on the Fitted vs. Residuals plot, QQ-Plot, and Shapiro-Wilkes test performed earlier prior to model selection, we know that these assumptions do not hold true. For example, we cannot assume equal variance of residual for all predictor values, and we cannot assume that the residuals follow a normal distribution when we are already confident that they do not. We can also acknowledge that our models, while at time complex, did not utilize transformations using logarithms or exponents. However, we don't feel that this was warranted given our findings, or that their inclusion would significantly impact the models' accuracies or our ultimate recommendation. The highest accuracy interactive models selected (i.e. `bic_back_mod` and `all_intr_mod`) likely suffer from overfitting due to their relative complexity. Steps to mitigate this in future studies are recommended further in this document.

Given the findings from two different models (including our most predictive model), given that that the models are highly-complex and somewhat challenging to interpret (due to their complexity and possible overfitting), and combined with the knowledge that the underlying data does not follow the typical assumptions of a linear model, **we recommend that our company pursue developing and releasing video games for the North American market for Nintendo platforms, and in the genres of "Sports", "Shooter", and "Role-Playing".** This is based on the significance of terms associated with these genres as well as with the Wii and DS (a portable Nintendo system) platforms. This recommendation is informed by our knowledge that this dataset is also limited to video game sales prior to 2016, and would have no knowledge of current Nintendo platforms such as the Nintendo Switch. It is also important to note that while `PlatformX360` was found to be a statistically-significant predictor in our best-performing model, this knowledge does not significantly influence our decision as the Xbox 360 has reached end-of-support from Microsoft and was subsequently replaced with the Xbox One and again with the Xbox Series X. Given the success of multiple Nintendo platforms we are confident in our recommendation.

We also recommend concurrently that our data analysis team find, extract, and repeat this model using new data which incorporates video game sales from 2016 to 2023. We would expect new models to suggest growth potential in Nintendo platforms in these genres consistent with our findings. The new model should also leverage new datasets whenever possible. The models created in our experiments were limited, and consisted largely of categorical or character values while linear regressions perform optimally with continuous numerical data. Information such as user surveys or video game review scores may aid to research in this manner. Mitigations for potential overfitting in future iterations and experiments include multiple-fold cross-validation (CV). This would likely result in lower overall accuracy but would carry the added benefit of the selected models better generalizing to unseen data.


--- 

## Appendix

Group Members:
### Ethan Cook, Brian Betancourt, Zongyu Li

