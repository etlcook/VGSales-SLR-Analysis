---
title: "STAT420 Zobreth Group Project"
author: "Brian Betancourt, Ethan Cook, Zongyu Li"
date: "2023-07-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# North American Video Game Sales Prediction

## Introduction
In this project, we assume the role of a video game company seeking to release games in the North American market for the first time. Given historical data regarding video game sales from other companies, we will use this data to predict our own successes (or follies) in the North American market and provide an informed decision to our company leadership regarding the potential risks in doing so.

### Exploring The Data


```{r}
library(faraway)
library(car)
vgsales = read.csv("vgsales.csv")
```

The data is stored as a `csv` file containing `r nrow(vgsales)` rows across `r ncol(vgsales)` columns (or predictors), and includes data from 1980 to 2020.


The data is largely "clean" and does not contain missing or "null" values, with the exception of the `Year` column which contains approximately `1.63%` null values.

```{r}
# null values per column
na_count = colSums(is.na(vgsales))
# proportion of NA values in each column
na_proportion = na_count / nrow(vgsales) * 100
# table/dataframe for readability
na_records = data.frame(Count = na_count, Proportion = na_proportion)
print(na_records)
```

We will also check for any unconventional data (i.e. non-numeric or date types in the `Year` predictor). 

Given that there appear to be a low number of records (271, or approximately 1.63% of the total dataset), we cast `Year` to be a numeric datatype for ease of use in modeling and we filter our dataset to remove these N/A records.
```{r}
# check non-numeric values in Year
unique(vgsales$Year[!grepl("^\\d+$", vgsales$Year)])

# count of "N/A" values in Year column
na_count = sum(vgsales$Year == "N/A")
# proportion of "N/A" values in Year column
na_proportion = na_count / nrow(vgsales) * 100

# print count and proportion
print(paste("Count of 'N/A' in Year: ", na_count))
print(paste("Proportion of 'N/A' in Year (%): ", na_proportion))

# filter the dataset
vgsales = vgsales[vgsales$Year != "N/A",]

# cast datatype
vgsales$Year = as.numeric(vgsales$Year)
```

It appear also that our `Year` values seem to trail off after 2016, so we can remove records from the dataset where year is > 2016
```{r}
# aggregate by year
global_sales_year = aggregate(Global_Sales ~ Year, vgsales, sum)

# plot
barplot(height = global_sales_year$Global_Sales, names.arg = global_sales_year$Year, xlab = "Year", ylab = "Total Sales (in millions)",main = "Total Sales (Global) by Year", las = 2,cex.names = 0.7) 

# filter dataset
vgsales = subset(vgsales, Year <= 2016)

```

The predictors have the following datatypes:

```{r}
str(vgsales)
```
- **Rank**: 
- *Name*: This is the title of a given video game
- *Platform*: This corresponds to the console the video game is released for
**Note**: It is possible that a video game may be released for more than one console simultaneously.
- *Year*: The year the video game was released
- *Publisher*: Company which published the video game
- *NA_Sales*: Number of sales in North America (in *millions of units sold*)
- *EU_Sales*: Number of sales in Europe (in *millions of units sold*)
- *JP_Sales*: Number of sales in Japan (in *millions of units sold*)
- *Other_Sales*: Number of sales in regions of the world outside of North America, Europe, and Japan (in *millions of units sold*)
- *Global_Sales*: Total Number of sales, globally (in *millions of units sold*)

#### Data Tendency

*What do sales look like, on average, for each geographic area and based on platform?*
```{r}
mean_sales_platform = aggregate(cbind(EU_Sales, JP_Sales, Other_Sales) ~ Platform, vgsales, mean)
print(mean_sales_platform)
```

**Assuming our company only cares about platforms from 2010 onward, we can focus on newer systems**

Below we can observe the average sales for each platform from 2010 to 2016
```{r}
# utilizing domain knowledge to filter platforms to only newer ones
new_platforms = c('X360', 'Wii', 'WiiU', 'XOne', 'PS3', 'PS4', 'PSV', '3DS', 'PC')
  # Filter data frame to include only specified platforms
vgsales_filtered = vgsales[vgsales$Platform %in% new_platforms, ]

mean_sales_platform_filtered = aggregate(cbind(EU_Sales, JP_Sales, Other_Sales) ~ Platform, vgsales_filtered, mean)
print(mean_sales_platform_filtered)

```


Of the newer platforms, which are most popular in each geographic region?
```{r}
# Find the platforms with highest mean sales in each region
highest_EU_platform = mean_sales_platform_filtered$Platform[which.max(mean_sales_platform_filtered$EU_Sales)]
highest_JP_platform = mean_sales_platform_filtered$Platform[which.max(mean_sales_platform_filtered$JP_Sales)]
highest_Other_platform = mean_sales_platform_filtered$Platform[which.max(mean_sales_platform_filtered$Other_Sales)]

highest_EU_sales = max(mean_sales_platform_filtered$EU_Sales)
highest_JP_sales = max(mean_sales_platform_filtered$JP_Sales)
highest_Other_sales <- max(mean_sales_platform_filtered$Other_Sales)

cat("Platform with highest mean sales in EU: ", highest_EU_platform, " (", highest_EU_sales, ")", "\n")
cat("Platform with highest mean sales in JP: ", highest_JP_platform, " (", highest_JP_sales, ")", "\n")
cat("Platform with highest mean sales in Other regions: ", highest_Other_platform, "(", highest_Other_sales, ")", "\n")
```

#### Data Distribution


Sales by Genre
```{r}
library(ggplot2)

# Aggregate total sales
eusales_agg = aggregate(EU_Sales ~ Genre, vgsales, sum)
jpsales_agg = aggregate(JP_Sales ~ Genre, vgsales, sum)
othersales_agg = aggregate(Other_Sales ~ Genre, vgsales, sum)

# EU Sales
ggplot(eusales_agg, aes(x = Genre, y = EU_Sales)) +geom_bar(stat = "identity", fill = "lightblue", color = "black") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x = "Genre", y = "Sales (in millions)", title = "Total EU Sales by Genre")

# JP Sales
ggplot(jpsales_agg, aes(x = Genre, y = JP_Sales)) + geom_bar(stat = "identity", fill = "lightblue", color = "black") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x = "Genre", y = "Sales (in millions)", title = "Total JP Sales by Genre")

# Other Sales
ggplot(othersales_agg, aes(x = Genre, y = Other_Sales)) + geom_bar(stat = "identity", fill = "lightblue", color = "black") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x = "Genre", y = "Sales (in millions)", title = "Total Other Sales by Genre")

```



### Data Trends

Total Sales by Year and Geographic Region
```{r}
# convert year (currently chr)
vgsales$Year = as.numeric(vgsales$Year)

# 2010 onward filter
vgsales_current = vgsales[vgsales$Year >= 2010, ]

# Calculate total sales using the filtered data frame
total_sales_year = aggregate(cbind(EU_Sales, JP_Sales, Other_Sales) ~ Year, vgsales_current, sum)

library(ggplot2)
ggplot(total_sales_year, aes(x = as.numeric(Year))) +
  geom_line(aes(y = JP_Sales, color = "JP_Sales")) +
  geom_line(aes(y = EU_Sales, color = "EU_Sales")) +
  geom_line(aes(y = Other_Sales, color = "Other_Sales")) +
  labs(color = "Region", x="Year", y = "Total Sales", title = "Yearly Sales by Region")

```


Total Sales by Platform (2010-2020)
```{r}
library(ggplot2)

# year filter
vgsales_current = vgsales[vgsales$Year >= 2010 & vgsales$Year <= 2020, ]
total_sales_platform = aggregate(cbind(EU_Sales, JP_Sales, Other_Sales) ~ Platform, vgsales_current, sum)

ggplot(total_sales_platform, aes(x = Platform, y = JP_Sales)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text()) +
  labs(title = "Total JP Sales by Platform (2010-2020)", x = "Platform", y = "Sales")

ggplot(total_sales_platform, aes(x = Platform, y = EU_Sales)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text()) +
  labs(title = "Total EU Sales by Platform (2010-2020)", x = "Platform", y = "Sales")

ggplot(total_sales_platform, aes(x = Platform, y = Other_Sales)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text()) +
  labs(title = "Total Other Region Sales by Platform (2010-2020)", x = "Platform", y = "Sales")


```

## Methods
*This is the section where we fit and analyze models in order to represent the data*

### Pre-Processing
Several of our potential predictors are represented as characters, but we will want to convert them to factor variables. These include:
* Platform
* Genre
* Publisher

First we will filter our dataset to only include games and platforms from 2010 on, then make the character predictors into factors, and then drop levels from factor variables that have no observations (i.e. levels that are only associated with data before 2010, such as Super Nintendo game data).

```{r}
# Filter data by year and system platform
vgsales_modern = vgsales[vgsales$Platform %in% new_platforms & vgsales$Year >= 2010 & vgsales$Year <= 2020, ]

# Factorize string predictors
vgsales_modern$Platform = as.factor(vgsales_modern$Platform)
vgsales_modern$Genre = as.factor(vgsales_modern$Genre)
vgsales_modern$Publisher = as.factor(vgsales_modern$Publisher)

# Remove unused factor levels
vgsales_modern = droplevels(vgsales_modern)
```

I have also decided to  combine EU_Sales, JP_Sales, and Other_Sales into a new predictor called Non_NA_Sales This is because The sales in different countries are correlated, and Global_Sales is highly correlated with the others since it is a summation, which also includes NA_Sales. I am removing the combined columns for better viewing.

```{r}
# combine sales predictors into one
Non_NA_Sales = vgsales_modern$EU_Sales + vgsales_modern$JP_Sales + vgsales_modern$Other_Sales
vgs_modern = cbind(vgsales_modern, Non_NA_Sales)
vgs_modern = subset(vgsales_modern, select = -c(EU_Sales, JP_Sales, Other_Sales))

str(vgs_modern)
```

*Note:* The Year predictor will be tried as a numerical variable, but for our smaller range of years a factor might be more appropriate.

### Model Selection
*In this section, we choose a model which can represent the data most accurately*
- Perform any tests necessary to find correlation between variables (i.e. correlation matrix)

we will generate several models and compare metrics to give us a starting point on intuition.

```{r}
all_intr_mod = lm(NA_Sales ~ Platform * Year * Genre * Non_NA_Sales + Publisher, data = vgs_modern)
all_mod = lm(NA_Sales ~ Platform + Year + Genre + Publisher + Non_NA_Sales, data = vgs_modern)
foreign_sales_mod = lm(NA_Sales ~ Non_NA_Sales, data = vgs_modern)
intuitive_mod = lm(NA_Sales ~ Non_NA_Sales * Platform + Non_NA_Sales * Genre + Year, data = vgs_modern)
intuitive_mod2 = lm(NA_Sales ~ Non_NA_Sales * Platform + Non_NA_Sales * Genre + Non_NA_Sales * Publisher + Year, data = vgs_modern)
bic_back_mod = step(all_intr_mod, direction = "backward", trace = FALSE, k = log(nrow(vgs_modern)))
```

Now we'll analyze these models and create a table

```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

metrics_table = data.frame(
        Model = c("all_intr_mod", "all_mod", "foreign_sales_mod", "intuitive_mod", "intuitive_mod2", "bic_back_mod"),
        R_Squared = c(summary(all_intr_mod)$adj.r.squared,
                    summary(all_mod)$adj.r.squared,
                    summary(foreign_sales_mod)$adj.r.squared,
                    summary(intuitive_mod)$adj.r.squared,
                    summary(intuitive_mod2)$adj.r.squared,
                    summary(bic_back_mod)$adj.r.squared),
      RMSE = c(sqrt(mean(resid(all_intr_mod) ^ 2)),
              sqrt(mean(resid(all_mod) ^ 2)),
              sqrt(mean(resid(foreign_sales_mod) ^ 2)),
              sqrt(mean(resid(intuitive_mod) ^ 2)),
              sqrt(mean(resid(intuitive_mod2) ^ 2)),
              sqrt(mean(resid(bic_back_mod)) ^ 2))
      )


metrics_table
```

Looking at the above models and their $R^2$ values, we see the large model with many fully interacting predictors has the best metric. This is unsurprising, since adding more and more predictors and interactions generally produces better predictions, but can become confusing and loses ability to interpret meaning between variables.



*It might make sense to fit a few models here:*
- Large Additive Model with all Variables `lm( y ~ ., data=data)`
- Large Model with all possible interactions `lm( y ~ .^2, data=data)`
- Models informed by an intuition or Hypothesis (our instinct is to say that variable y is likely to be predictive)

*Consider evaluating the assumptions of the model/data to see if they hold true*
*Consider the implications for our model if they do not hold true*
- Q-Q Plot
- Shapiro Test

*Consider optimizing feature selection using stepwise AIC/BIC and be able to explain why this model should be the best*
*Comment on any predictor interactions or transformations used*

## Results
*Numerical or graphical summaries of your results.*
*You should report a final model you have chosen. There is not necessarily one, singular correct model, but certainly some methods and models are better than others in certain situations. You may use any methods we studied this semester to complete this task, and provide evidence that your final choice of model is a good one. *

In the following figures, we show bar plots for the R^2 values and RMSE. We observe that for "all_intr_mod", "foreign_sales_mod" and "intuitive_mod2" and "bic_back_mod" have the highest R^2 values. For the RMSE bar plot, it's interesting to observe that the "foreign_sales_mod" has RMSE closer to 0, while "all_intr_mod", "intuitive_mod2" and "bic_back_mod" have slight higher RMSE values. 
```{r}
# Install and load required libraries
# Install and load required libraries (if not already installed and loaded)
install.packages("tidyverse")
library(tidyverse)

# Given data
data <- data.frame(
  Model = c("all_intr_mod", "all_mod", "foreign_sales_mod", "intuitive_mod", "intuitive_mod2", "bic_back_mod"),
  R_Squared = c(0.8531991, 0.6146540, 0.5695697, 0.8135079, 0.8532400, 0.8254627),
  RMSE = c(2.380367e-01, 4.064571e-01, 4.427398e-01, 2.900292e-01, 2.455912e-01, 5.198306e-17)
)

# Reshape the data to a long format using gather (tidyr)
data_long <- data %>%
  gather(metric, value, -Model)

# Install and load required libraries (if not already installed and loaded)
install.packages("ggplot2")
library(ggplot2)

# Bar plot for R_Squared
ggplot(data_long[data_long$metric == "R_Squared", ], aes(x = Model, y = value)) +
  geom_col(fill = "blue") +
  labs(x = "Model", y = "R_Squared") +
  ggtitle("R_Squared for Different Models") +
  theme_minimal()

# Bar plot for RMSE
ggplot(data_long[data_long$metric == "RMSE", ], aes(x = Model, y = value)) +
  geom_col(fill = "green") +
  labs(x = "Model", y = "RMSE") +
  ggtitle("RMSE for Different Models") +
  theme_minimal()

```

To conduct a comprehensive comparison of the four models, namely "all_intr_mod," "foreign_sales_mod," "intuitive_mod2," and "bic_back_mod," we conducted ANOVA tests on each of them. Our primary objective was to determine if the statistical evidence from the ANOVA tests would support the notion that "bic_back_mod" exhibits superior performance when compared to the other three models. From the following tables, we see that all p-values are significant. This means that the "bic_back_mod" is statistically superior to the other three models, "all_intr_mod", "foreign_sales_mod" and "intuitive_mod2" based on the ANOVA tests.
```{r}
anova_results_all_intr <- anova(all_intr_mod, bic_back_mod)
anova_results_foreign_sales <- anova(foreign_sales_mod, bic_back_mod)
anova_results_intuitive <- anova(intuitive_mod2, bic_back_mod)

# Print the ANOVA tables
print("ANOVA test results for all_intr_mod and bic_back_mod:")
print(anova_results_all_intr)

print("ANOVA test results for foreign_sales_mod and bic_back_mod:")
print(anova_results_foreign_sales)

print("ANOVA test results for intuitive_mod2 and bic_back_mod:")
print(anova_results_intuitive)

```

The "bic_back_mod" model's improved performance can be attributed to its inclusion of the interaction terms (Platform:Year, Platform:Non_NA_Sales, Year:Non_NA_Sales, and Genre:Non_NA_Sales), which allowed for a more accurate and nuanced representation of the relationship between the predictors and the response variable, NA_Sales. Moreover, the incorporation of Non_NA_Sales as an additional predictor further enhanced the model's predictive capabilities.

## Discussion
In the context of our original business problem, the results indicate that it (IS/IS NOT) in the interest of our video game company to enter the North American market. This conclusion is informed by:
- Reason A
- Reason B
- Reason C

*In this paragraph, we can explain Reason A* We can use graphics here as necessary, and we can explain the significance of this graph/test/analysis in the context of our company.

*In this paragraph, we can explain Reason B* We can use graphics here as necessary, and we can explain the significance of this graph/test/analysis in the context of our company.

*In this paragraph, we can explain Reason C* We can use graphics here as necessary, and we can explain the significance of this graph/test/analysis in the context of our company.

## Appendix
Should contain code and analysis that is used, but that would have otherwise cluttered the report or is not directly related to the choice of model. Do not simply dump code in here. Only utilize the appendix to supplement the primary focus of the report. The appendix should also conclude with the names of the group members.

```{r}

head(vgsales)
```


