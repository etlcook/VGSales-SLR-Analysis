---
title: "STAT420 Zobreth Group Project"
author: "Brian Betancourt, Ethan Cook, Zongyu Li"
date: "2023-07-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# North American Video Game Sales Prediction

## Introduction
In this project, we assume the role of a video game company seeking to release games in the North American market for the first time. Given historical data regarding video game sales from other companies, we will use this data to predict our own successes (or follies) in the North American market and provide an informed decision to our company leadership regarding the potential risks in doing so.

### Exploring The Data


```{r}
library(faraway)
library(car)
vgsales = read.csv("vgsales.csv")
```

The data is stored as a `csv` file containing `r nrow(vgsales)` rows across `r ncol(vgsales)` columns (or predictors), and includes data from 1980 to 2020.


The data is largely "clean" and does not contain missing or "null" values, with the exception of the `Year` column which contains approximately `1.63%` null values.

```{r}
# null values per column
na_count = colSums(is.na(vgsales))
# proportion of NA values in each column
na_proportion = na_count / nrow(vgsales) * 100
# table/dataframe for readability
na_records = data.frame(Count = na_count, Proportion = na_proportion)
print(na_records)
```

We will also check for any unconventional data (i.e. non-numeric or date types in the `Year` predictor). 

Given that there appear to be a low number of records (271, or approximately 1.63% of the total dataset), we cast `Year` to be a numeric datatype for ease of use in modeling and we filter our dataset to remove these N/A records.
```{r}
# check non-numeric values in Year
unique(vgsales$Year[!grepl("^\\d+$", vgsales$Year)])

# count of "N/A" values in Year column
na_count = sum(vgsales$Year == "N/A")
# proportion of "N/A" values in Year column
na_proportion = na_count / nrow(vgsales) * 100

# print count and proportion
print(paste("Count of 'N/A' in Year: ", na_count))
print(paste("Proportion of 'N/A' in Year (%): ", na_proportion))

# filter the dataset
vgsales = vgsales[vgsales$Year != "N/A",]

# cast datatype
vgsales$Year = as.numeric(vgsales$Year)
```

It appear also that our `Year` values seem to trail off after 2016, so we can remove records from the dataset where year is > 2016
```{r}
# aggregate by year
global_sales_year = aggregate(Global_Sales ~ Year, vgsales, sum)

# plot
barplot(height = global_sales_year$Global_Sales, names.arg = global_sales_year$Year, xlab = "Year", ylab = "Total Sales (in millions)",main = "Total Sales (Global) by Year", las = 2,cex.names = 0.7) 

# filter dataset
vgsales = subset(vgsales, Year <= 2016)

```

The predictors have the following datatypes:

```{r}
str(vgsales)
```
- **Rank**: 
- *Name*: This is the title of a given video game
- *Platform*: This corresponds to the console the video game is released for
**Note**: It is possible that a video game may be released for more than one console simultaneously.
- *Year*: The year the video game was released
- *Publisher*: Company which published the video game
- *NA_Sales*: Number of sales in North America (in *millions of units sold*)
- *EU_Sales*: Number of sales in Europe (in *millions of units sold*)
- *JP_Sales*: Number of sales in Japan (in *millions of units sold*)
- *Other_Sales*: Number of sales in regions of the world outside of North America, Europe, and Japan (in *millions of units sold*)
- *Global_Sales*: Total Number of sales, globally (in *millions of units sold*)

#### Data Tendency

*What do sales look like, on average, for each geographic area and based on platform?*
```{r}
mean_sales_platform = aggregate(cbind(EU_Sales, JP_Sales, Other_Sales) ~ Platform, vgsales, mean)
print(mean_sales_platform)
```

**Assuming our company only cares about platforms from 2010 onward, we can focus on newer systems**

Below we can observe the average sales for each platform from 2010 to 2016
```{r}
# utilizing domain knowledge to filter platforms to only newer ones
new_platforms = c('X360', 'Wii', 'WiiU', 'XOne', 'PS3', 'PS4', 'PSV', '3DS', 'PC')
  # Filter data frame to include only specified platforms
vgsales_filtered = vgsales[vgsales$Platform %in% new_platforms, ]

mean_sales_platform_filtered = aggregate(cbind(EU_Sales, JP_Sales, Other_Sales) ~ Platform, vgsales_filtered, mean)
print(mean_sales_platform_filtered)

```


Of the newer platforms, which are most popular in each geographic region?
```{r}
# Find the platforms with highest mean sales in each region
highest_EU_platform = mean_sales_platform_filtered$Platform[which.max(mean_sales_platform_filtered$EU_Sales)]
highest_JP_platform = mean_sales_platform_filtered$Platform[which.max(mean_sales_platform_filtered$JP_Sales)]
highest_Other_platform = mean_sales_platform_filtered$Platform[which.max(mean_sales_platform_filtered$Other_Sales)]

highest_EU_sales = max(mean_sales_platform_filtered$EU_Sales)
highest_JP_sales = max(mean_sales_platform_filtered$JP_Sales)
highest_Other_sales <- max(mean_sales_platform_filtered$Other_Sales)

cat("Platform with highest mean sales in EU: ", highest_EU_platform, " (", highest_EU_sales, ")", "\n")
cat("Platform with highest mean sales in JP: ", highest_JP_platform, " (", highest_JP_sales, ")", "\n")
cat("Platform with highest mean sales in Other regions: ", highest_Other_platform, "(", highest_Other_sales, ")", "\n")
```

#### Data Distribution


Sales by Genre
```{r}
library(ggplot2)

# Aggregate total sales
eusales_agg = aggregate(EU_Sales ~ Genre, vgsales, sum)
jpsales_agg = aggregate(JP_Sales ~ Genre, vgsales, sum)
othersales_agg = aggregate(Other_Sales ~ Genre, vgsales, sum)

# EU Sales
ggplot(eusales_agg, aes(x = Genre, y = EU_Sales)) +geom_bar(stat = "identity", fill = "lightblue", color = "black") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x = "Genre", y = "Sales (in millions)", title = "Total EU Sales by Genre")

# JP Sales
ggplot(jpsales_agg, aes(x = Genre, y = JP_Sales)) + geom_bar(stat = "identity", fill = "lightblue", color = "black") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x = "Genre", y = "Sales (in millions)", title = "Total JP Sales by Genre")

# Other Sales
ggplot(othersales_agg, aes(x = Genre, y = Other_Sales)) + geom_bar(stat = "identity", fill = "lightblue", color = "black") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x = "Genre", y = "Sales (in millions)", title = "Total Other Sales by Genre")

```



### Data Trends

Total Sales by Year and Geographic Region
```{r}
# convert year (currently chr)
vgsales$Year = as.numeric(vgsales$Year)

# 2010 onward filter
vgsales_current = vgsales[vgsales$Year >= 2010, ]

# Calculate total sales using the filtered data frame
total_sales_year = aggregate(cbind(EU_Sales, JP_Sales, Other_Sales) ~ Year, vgsales_current, sum)

library(ggplot2)
ggplot(total_sales_year, aes(x = as.numeric(Year))) +
  geom_line(aes(y = JP_Sales, color = "JP_Sales")) +
  geom_line(aes(y = EU_Sales, color = "EU_Sales")) +
  geom_line(aes(y = Other_Sales, color = "Other_Sales")) +
  labs(color = "Region", x="Year", y = "Total Sales", title = "Yearly Sales by Region")

```


Total Sales by Platform (2010-2020)
```{r}
library(ggplot2)

# year filter
vgsales_current = vgsales[vgsales$Year >= 2010 & vgsales$Year <= 2020, ]
total_sales_platform = aggregate(cbind(EU_Sales, JP_Sales, Other_Sales) ~ Platform, vgsales_current, sum)

ggplot(total_sales_platform, aes(x = Platform, y = JP_Sales)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text()) +
  labs(title = "Total JP Sales by Platform (2010-2020)", x = "Platform", y = "Sales")

ggplot(total_sales_platform, aes(x = Platform, y = EU_Sales)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text()) +
  labs(title = "Total EU Sales by Platform (2010-2020)", x = "Platform", y = "Sales")

ggplot(total_sales_platform, aes(x = Platform, y = Other_Sales)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text()) +
  labs(title = "Total Other Region Sales by Platform (2010-2020)", x = "Platform", y = "Sales")


```

## Methods
*This is the section where we fit and analyze models in order to represent the data*

### Pre-Processing
Several of our potential predictors are represented as characters, but we will want to convert them to factor variables. These include:  
* Platform  
* Genre  
* Publisher  
  
Previously we have filtered our dataset to only include games and platforms from 2010 on; next we'll make the character predictors into factors, then drop levels from factor variables that have no observations (i.e. levels that are only associated with data before 2010, such as Super Nintendo game data).

```{r}
# Factorize string predictors
vgsales_current$Platform = as.factor(vgsales_current$Platform)
vgsales_current$Genre = as.factor(vgsales_current$Genre)
vgsales_current$Publisher = as.factor(vgsales_current$Publisher)

# Remove unused factor levels
vgsales_modern = droplevels(vgsales_current)
```

I have also decided to  combine EU_Sales, JP_Sales, and Other_Sales into a new predictor called Non_NA_Sales This is because The sales in different countries are correlated, and Global_Sales is highly correlated with the others since it is an aggregation, which also includes NA_Sales. I am removing the combined columns for better viewing.

```{r}
# combine sales predictors into one
Non_NA_Sales = vgsales_modern$EU_Sales + vgsales_modern$JP_Sales + vgsales_modern$Other_Sales
vgs_modern = cbind(vgsales_modern, Non_NA_Sales)
vgs_modern = subset(vgsales_modern, select = -c(EU_Sales, JP_Sales, Other_Sales))

str(vgs_modern)
```

*Note:* The Year predictor will be tried as a numerical variable, but for our smaller range of years a factor might be more appropriate.

### Model Selection
*In this section, we choose a model which can represent the data most accurately*
- Perform any tests necessary to find correlation between variables (i.e. correlation matrix)

we will generate several models and compare metrics to give us a starting point on intuition.

```{r}
all_intr_mod = lm(NA_Sales ~ Platform * Year * Genre * Non_NA_Sales + Publisher, data = vgs_modern)
all_mod = lm(NA_Sales ~ Platform + Year + Genre + Publisher + Non_NA_Sales, data = vgs_modern)
foreign_sales_mod = lm(NA_Sales ~ Non_NA_Sales, data = vgs_modern)
intuitive_mod = lm(NA_Sales ~ Non_NA_Sales * Platform + Non_NA_Sales * Genre + Year, data = vgs_modern)
intuitive_mod2 = lm(NA_Sales ~ Non_NA_Sales * Platform + Non_NA_Sales * Genre + Non_NA_Sales * Publisher + Year, data = vgs_modern)
bic_back_mod = step(all_intr_mod, direction = "backward", trace = FALSE, k = log(nrow(vgs_modern)))
```

The intuitive models above were chosen using experience as a customer of video games and a previous salesman. The first, "intuitive_mod", posits that there are interactions between global non-NA sales and the platform/genre of the game. Platforms and genres can vary a lot in popularity and so I am accounting for different coefficients needed for the different factor levels.

The 2nd intuitive model, "intuitive_mod2", is similar to the first but includes an interaction between global non-NA sales and the video game publisher. This is based off of the understanding that there are big-name publishers that always produce more sales (profit not being a factor) and smaller independent publishers with more modest sales. As seen in later tests, this interaction gives the model a large boost in prediction accuracy without making it much more confusing.

With our models generated, let's get information on violated assumptions before producing results. I'll use the two models with the best prediction and interpretability combinations (metrics on prediction in later charts).
```{r}
par(mfrow = c(1, 2))
plot(fitted(intuitive_mod2), resid(intuitive_mod2), xlab = "Fitted", ylab = "Residuals", main = "Intuitive Model 2 Fit vs. Resid")
plot(fitted(bic_back_mod), resid(bic_back_mod), xlab = "Fitted", ylab = "Residuals", main = "BIC Backward Model Fit vs. Resid")
```

We see similar charts where there is a lot of variability regardless of fitted magnitude, but the less popularly selling games tend to be easier to predict. This also shows that the distribution of our data is far from normal, which we'll confirm below.

Now we will test the assumptions of a SLR model. Generating a Q-Q plot for each model yeilds very similar results, so I will only include the graph for the better performing intuitive model 2:
```{r}
qqnorm(resid(intuitive_mod2))
qqline(resid(intuitive_mod2), col = "red", lwd = 2)
stest = shapiro.test(resid(intuitive_mod2)[1:5000])
```
Clearly, the errors of our model are not normally distributed, as the plot shows trailing tails at both ends. The Shapiro-Wilkes test concurs by giving an extremely small p-value of `r stest$p.value`. *Note: we had to exclude 141 of the 5141 residuals dues to limitations of the shapiro.test function*.

## Results
*Numerical or graphical summaries of your results.*
*You should report a final model you have chosen. There is not necessarily one, singular correct model, but certainly some methods and models are better than others in certain situations. You may use any methods we studied this semester to complete this task, and provide evidence that your final choice of model is a good one. *


## Discussion
In the context of our original business problem, the results indicate that it (IS/IS NOT) in the interest of our video game company to enter the North American market. This conclusion is informed by:
- Reason A
- Reason B
- Reason C

*In this paragraph, we can explain Reason A* We can use graphics here as necessary, and we can explain the significance of this graph/test/analysis in the context of our company.

*In this paragraph, we can explain Reason B* We can use graphics here as necessary, and we can explain the significance of this graph/test/analysis in the context of our company.

*In this paragraph, we can explain Reason C* We can use graphics here as necessary, and we can explain the significance of this graph/test/analysis in the context of our company.

## Appendix
Should contain code and analysis that is used, but that would have otherwise cluttered the report or is not directly related to the choice of model. Do not simply dump code in here. Only utilize the appendix to supplement the primary focus of the report. The appendix should also conclude with the names of the group members.

### Ethan Cook, Brian Betancourt, Zongyu Li
